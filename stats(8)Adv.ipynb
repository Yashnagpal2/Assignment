{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e258813c-b82a-4c4e-9683-b85228b47a06",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afb485-60e0-4d48-b4d0-ffcad57dae8d",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical technique used to compare the means of three or more groups to determine if there are statistically significant differences among them. ANOVA is based on several assumptions that, if violated, can impact the validity of the results. Here are the key assumptions for ANOVA:\n",
    "\n",
    "Independence of Observations: The observations in each group or treatment level should be independent of each other. Violations can occur when observations within a group are correlated, such as in time-series data or repeated measures designs. For example, if you're comparing the exam scores of students who were taught by the same teacher, the scores may not be independent if teacher quality impacts multiple students.\n",
    "\n",
    "Normality: The residuals (the differences between the observed values and the group means) should be normally distributed. Violations occur when the residuals are not normally distributed. For example, if you're comparing the test scores of different schools, and the scores are heavily skewed or show a non-normal pattern, this assumption is violated.\n",
    "\n",
    "Homogeneity of Variance (Homoscedasticity): The variances of each group or treatment level should be roughly equal. In other words, the spread of the data should be consistent across groups. Violations occur when the variances are not equal, leading to unequal dispersion of data points. For instance, if you're comparing the weight loss of participants under different diet plans, and one diet plan has significantly more variation in weight loss, this assumption is violated.\n",
    "\n",
    "Mutual Independence: This assumption is more relevant in two-way or multi-way ANOVA. It assumes that the residuals are independent across all combinations of the factors or variables being studied. Violations occur when there are dependencies between factors or interactions that are not accounted for in the analysis.\n",
    "\n",
    "Examples of Violations:\n",
    "\n",
    "Non-Normality: If you're comparing the average monthly income of employees from different departments, and the income data for one department is highly skewed or doesn't follow a normal distribution, the normality assumption is violated.\n",
    "\n",
    "Heteroscedasticity: Suppose you're conducting a study to compare the effectiveness of different drugs on patients with a certain condition, and the variation in patient responses for one drug is much larger than for the others. This would violate the assumption of homogeneity of variance.\n",
    "\n",
    "Correlated Observations: In a study comparing the effects of different types of fertilizer on plant growth, if the plants in the same garden are more similar in growth than plants from different gardens, the assumption of independence is violated.\n",
    "\n",
    "When these assumptions are violated, it can lead to inaccurate or unreliable results. In such cases, it may be necessary to explore alternative statistical methods or transform the data to meet the assumptions or conduct non-parametric tests, which are more robust to violations of these assumptions. Additionally, it's essential to report any assumptions that were violated and discuss their potential impact on the interpretation of the results in research or experimental settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c64c0a-6c46-45da-b693-9ddc5285124a",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736f1e5-83f2-4df3-b386-ffc4f12ffb18",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical technique used to compare the means of three or more groups to determine if there are statistically significant differences among them. There are three main types of ANOVA, each designed for specific situations:\n",
    "\n",
    "One-Way ANOVA (One-Factor ANOVA):\n",
    "\n",
    "Situation: One-Way ANOVA is used when you have one categorical independent variable (factor) with more than two levels or groups, and you want to determine if there are any statistically significant differences among the group means.\n",
    "Example: An experiment that tests the effects of different doses of a drug (e.g., low, medium, high) on blood pressure. Here, the independent variable is the dose, and the dependent variable is blood pressure.\n",
    "Two-Way ANOVA (Two-Factor ANOVA):\n",
    "\n",
    "Situation: Two-Way ANOVA is used when you have two categorical independent variables (factors) and you want to assess their main effects and potential interactions on a continuous dependent variable. It's typically used when you have a factorial design with two factors.\n",
    "Example: A study investigating the effects of both the type of diet (e.g., low-carb, high-carb) and the type of exercise (e.g., aerobic, strength) on weight loss. Here, the independent variables are diet and exercise, and the dependent variable is weight loss.\n",
    "Repeated Measures ANOVA:\n",
    "\n",
    "Situation: Repeated Measures ANOVA is used when you have a within-subject design, and you want to compare the means of a single group under different conditions or time points. It's suitable when each subject is measured multiple times or under multiple conditions.\n",
    "Example: An experiment that measures the blood pressure of the same individuals before and after receiving different treatments (e.g., placebo, drug A, drug B). The independent variable is the treatment, and the dependent variable is blood pressure. Since each subject is measured multiple times, a repeated measures ANOVA is used.\n",
    "In summary:\n",
    "\n",
    "One-Way ANOVA is used for one categorical independent variable with multiple levels.\n",
    "Two-Way ANOVA is used for two categorical independent variables, often in a factorial design.\n",
    "Repeated Measures ANOVA is used when you measure the same subjects under multiple conditions or time points.\n",
    "The choice of which type of ANOVA to use depends on the experimental design and research questions. It's important to select the appropriate ANOVA method to ensure the validity of the statistical analysis and the relevance of the results to your research objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e2fa1-0ef8-47ba-aff2-3bf180c68ae7",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e28852-bcc8-405f-8613-5f72dffba35f",
   "metadata": {},
   "source": [
    "The partitioning of variance in Analysis of Variance (ANOVA) refers to the decomposition of the total variability in the data into different components or sources of variation. Understanding this concept is crucial because it allows researchers to dissect and quantify the sources of variability in a statistical model, which, in turn, helps in assessing the significance of the effects and making meaningful inferences about the data. The key components of variance in ANOVA include:\n",
    "\n",
    "Total Variance (SST): Total variance represents the overall variability in the data. It is the sum of squares of the differences between each data point and the overall mean. SST = SSBetween + SSWithin, where SSBetween is the variance between groups and SSWithin is the variance within groups.\n",
    "\n",
    "Between-Group Variance (SSBetween): This component of variance represents the variability between different groups or treatment levels in the study. It is calculated as the sum of squares of the differences between the group means and the overall mean.\n",
    "\n",
    "Within-Group Variance (SSWithin): Within-group variance accounts for the variability within each group or treatment level. It is the sum of squares of the differences between individual data points and their respective group means.\n",
    "\n",
    "The importance of understanding the partitioning of variance in ANOVA lies in several aspects:\n",
    "\n",
    "Hypothesis Testing: ANOVA is used to test whether the means of different groups are significantly different from each other. By partitioning the variance into between-group and within-group components, ANOVA helps determine whether the differences between groups (SSBetween) are greater than the variation within groups (SSWithin). If the between-group variation is significantly larger, it suggests that the groups are different from each other.\n",
    "\n",
    "Effect Size: By quantifying the proportion of variance attributed to the independent variable (effect) and the residual variance, researchers can assess the practical significance or effect size of the treatment or factor being studied.\n",
    "\n",
    "Interpretation: Understanding the partitioning of variance allows researchers to interpret the contributions of different factors and assess the impact of each factor on the dependent variable. This is crucial for making informed decisions and drawing meaningful conclusions from the data.\n",
    "\n",
    "Model Assessment: It helps in evaluating the goodness-of-fit of the statistical model. Researchers can assess whether the model adequately accounts for the observed variability in the data.\n",
    "\n",
    "Error Detection: It can help in identifying sources of error or unexplained variability, which may lead to insights for further investigation or refinement of experimental designs.\n",
    "\n",
    "Assumptions: ANOVA relies on assumptions related to the variance components. Understanding how variance is partitioned helps researchers assess whether these assumptions are met or whether transformations or alternative methods are needed.\n",
    "\n",
    "In summary, the partitioning of variance in ANOVA is essential for hypothesis testing, effect size estimation, model interpretation, and the overall assessment of the validity and significance of the results. It provides a structured framework for evaluating the impact of independent variables on the dependent variable and guides researchers in drawing meaningful conclusions from their experiments.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e05d1-0163-47e8-93cf-c0b2e203b1d7",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c159d3-6514-44be-899f-b5a57336bc24",
   "metadata": {},
   "source": [
    "In a one-way Analysis of Variance (ANOVA), you can calculate the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) using Python. Here's how you can do it:\n",
    "\n",
    "Assume you have a dataset with one independent variable (factor) that divides your data into 'k' groups, and you want to perform a one-way ANOVA. You will calculate these sums of squares:\n",
    "\n",
    "SST (Total Sum of Squares): This represents the total variation in the dependent variable.\n",
    "\n",
    "SSE (Explained Sum of Squares): This represents the variation explained by the factor or group differences.\n",
    "\n",
    "SSR (Residual Sum of Squares): This represents the unexplained variation or variation within each group.\n",
    "\n",
    "Here's Python code to calculate SST, SSE, and SSR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25e8334-c82b-42be-997e-a325ed2e62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST: 710.1\n",
      "SSE: 592.9000000000004\n",
      "SSR: 117.19999999999959\n",
      "F-statistic: 40.47098976109232\n",
      "p-value: 0.00021786786431998717\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for each group\n",
    "group1 = np.array([60, 65, 70, 62, 58])  # Replace with your actual data for Group 1\n",
    "group2 = np.array([75, 78, 80, 82, 77])  # Replace with your actual data for Group 2\n",
    "# ... (add data for all groups)\n",
    "\n",
    "# Calculate the overall mean (grand mean)\n",
    "all_data = np.concatenate([group1, group2])  # Combine data from all groups\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate SST (Total Sum of Squares)\n",
    "SST = np.sum((all_data - overall_mean) ** 2)\n",
    "\n",
    "# Calculate SSE (Explained Sum of Squares)\n",
    "group_means = [np.mean(group) for group in [group1, group2]]  # Calculate means for each group\n",
    "SSE = np.sum([len(group) * (group_mean - overall_mean) ** 2 for group, group_mean in zip([group1, group2], group_means)])\n",
    "\n",
    "# Calculate SSR (Residual Sum of Squares)\n",
    "SSR = SST - SSE\n",
    "\n",
    "# Degrees of freedom\n",
    "df_total = len(all_data) - 1\n",
    "df_groups = len(group_means) - 1\n",
    "df_residual = df_total - df_groups\n",
    "\n",
    "# F-statistic and p-value\n",
    "F_statistic = (SSE / df_groups) / (SSR / df_residual)\n",
    "p_value = 1 - stats.f.cdf(F_statistic, df_groups, df_residual)\n",
    "\n",
    "print(\"SST:\", SST)\n",
    "print(\"SSE:\", SSE)\n",
    "print(\"SSR:\", SSR)\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9016ba0-fe75-4c14-bb88-5fa121f3e939",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a94018-3e4f-4a24-a51f-e6c95d4a17f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect A F-statistic: -1.0\n",
      "Main Effect A p-value: 1.0\n",
      "Main Effect B F-statistic: -1.0\n",
      "Main Effect B p-value: 1.0\n",
      "Interaction Effect F-statistic: 3.47722137268304e-15\n",
      "Interaction Effect p-value: 0.9999999535651058\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for each combination of factors A and B\n",
    "data_A1_B1 = np.array([82, 75, 78, 80, 77])\n",
    "data_A1_B2 = np.array([68, 70, 72, 74, 71])\n",
    "data_A2_B1 = np.array([90, 85, 88, 87, 89])\n",
    "data_A2_B2 = np.array([72, 75, 70, 74, 76])\n",
    "\n",
    "# Calculate the means for each combination of factors\n",
    "mean_A1_B1 = np.mean(data_A1_B1)\n",
    "mean_A1_B2 = np.mean(data_A1_B2)\n",
    "mean_A2_B1 = np.mean(data_A2_B1)\n",
    "mean_A2_B2 = np.mean(data_A2_B2)\n",
    "\n",
    "# Calculate the overall mean (grand mean)\n",
    "all_data = np.concatenate([data_A1_B1, data_A1_B2, data_A2_B1, data_A2_B2])\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate the main effects for factors A and B\n",
    "main_effect_A = ((mean_A1_B1 + mean_A1_B2) / 2 - overall_mean) + ((mean_A2_B1 + mean_A2_B2) / 2 - overall_mean)\n",
    "main_effect_B = ((mean_A1_B1 + mean_A2_B1) / 2 - overall_mean) + ((mean_A1_B2 + mean_A2_B2) / 2 - overall_mean)\n",
    "\n",
    "# Calculate the interaction effect\n",
    "interaction_effect = (mean_A1_B1 - (mean_A1_B1 + mean_A1_B2) / 2) + (mean_A1_B2 - (mean_A1_B1 + mean_A1_B2) / 2) + \\\n",
    "                    (mean_A2_B1 - (mean_A2_B1 + mean_A2_B2) / 2) + (mean_A2_B2 - (mean_A2_B1 + mean_A2_B2) / 2)\n",
    "\n",
    "# Degrees of freedom\n",
    "df_total = len(all_data) - 1\n",
    "df_A = 1  # Degrees of freedom for factor A (1 level of independence)\n",
    "df_B = 1  # Degrees of freedom for factor B (1 level of independence)\n",
    "df_interaction = df_A * df_B\n",
    "\n",
    "# F-statistics and p-values for main effects and interaction\n",
    "F_main_A = (main_effect_A / df_A) / (interaction_effect / df_interaction)\n",
    "F_main_B = (main_effect_B / df_B) / (interaction_effect / df_interaction)\n",
    "F_interaction = (interaction_effect / df_interaction) / (overall_mean / df_total)\n",
    "\n",
    "p_value_main_A = 1 - stats.f.cdf(F_main_A, df_A, df_interaction)\n",
    "p_value_main_B = 1 - stats.f.cdf(F_main_B, df_B, df_interaction)\n",
    "p_value_interaction = 1 - stats.f.cdf(F_interaction, df_interaction, df_total)\n",
    "\n",
    "print(\"Main Effect A F-statistic:\", F_main_A)\n",
    "print(\"Main Effect A p-value:\", p_value_main_A)\n",
    "print(\"Main Effect B F-statistic:\", F_main_B)\n",
    "print(\"Main Effect B p-value:\", p_value_main_B)\n",
    "print(\"Interaction Effect F-statistic:\", F_interaction)\n",
    "print(\"Interaction Effect p-value:\", p_value_interaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02343af5-648e-4f2b-b428-4e75eb38f52c",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475acbf-da98-4880-96c7-ef7ce56885e0",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are statistically significant differences between the means of three or more groups. The associated p-value helps determine the significance of these differences. When you obtain an F-statistic of 5.23 and a p-value of 0.02, here's how you can interpret the results:\n",
    "\n",
    "Significant F-Statistic:\n",
    "\n",
    "The F-statistic in your case is 5.23, which represents the ratio of the variation between group means to the variation within groups. A higher F-statistic indicates more substantial differences between groups.\n",
    "Low P-Value:\n",
    "\n",
    "The p-value is 0.02, which is less than the significance level (alpha), often set at 0.05. This means that the probability of obtaining such results by random chance is only 2%, which is lower than the chosen significance level.\n",
    "Conclusion:\n",
    "\n",
    "With an F-statistic significantly different from 1 and a low p-value, you can conclude that there are statistically significant differences between at least two of the groups in your data. In other words, at least one of the groups is different from the others in terms of the variable you analyzed.\n",
    "Post-hoc Tests:\n",
    "\n",
    "While ANOVA can tell you that there are differences between groups, it doesn't identify which specific groups are different from each other. To determine which groups differ, you may need to perform post-hoc tests, such as Tukey's HSD or Bonferroni tests. These tests will help you identify pairwise differences between groups.\n",
    "Effect Size:\n",
    "\n",
    "Consider evaluating the effect size to understand the practical significance of the differences. A higher F-statistic suggests a larger effect, but you can also calculate measures like eta-squared or Cohen's d to quantify the effect size.\n",
    "In summary, based on the F-statistic of 5.23 and the p-value of 0.02, you can conclude that there are statistically significant differences between the groups you compared in your one-way ANOVA. However, to determine the specific groups that differ, post-hoc tests are often necessary. Additionally, consider the effect size to gauge the practical significance of these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332baf24-3038-4efc-b917-0cb90ec1930f",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbda75-ec5d-4f0f-98d1-195464146a05",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is an important consideration to ensure the validity of your analysis. Repeated measures designs involve collecting data from the same subjects or experimental units at multiple time points or conditions. Missing data can arise for various reasons, and it's essential to deal with them appropriately. Here are some common methods for handling missing data and their potential consequences:\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion):\n",
    "\n",
    "Method: This approach involves excluding any subjects or cases with missing data, so only complete cases are used in the analysis.\n",
    "Consequences:\n",
    "Advantages: It's straightforward and preserves the available data.\n",
    "Disadvantages: Reduces the sample size, potentially leading to reduced statistical power and biased results if data are not missing completely at random (MCAR). It can also lead to less representative samples, especially if missing data are not random.\n",
    "Pairwise Deletion (Available Case Analysis):\n",
    "\n",
    "Method: With this method, you use all available data for each pairwise comparison, even if some data points are missing for specific comparisons.\n",
    "Consequences:\n",
    "Advantages: Maximizes the use of available data and can maintain a larger sample size for different comparisons.\n",
    "Disadvantages: Can produce biased results if missing data are not MCAR. The analysis might include different subsets of subjects for each comparison, potentially complicating interpretation.\n",
    "Imputation Methods:\n",
    "\n",
    "Method: Imputation involves estimating or filling in missing values using various techniques, such as mean imputation, regression imputation, or multiple imputation.\n",
    "Consequences:\n",
    "Advantages: Imputation allows you to retain the complete sample size and can improve the precision of estimates. It is particularly useful when missing data are missing at random (MAR) or when imputation assumptions are met.\n",
    "Disadvantages: The choice of imputation method can affect results. Inappropriate imputation methods or assumptions can lead to biased estimates. Additionally, imputation may not be appropriate for data that are not MAR.\n",
    "Mixed Models (Longitudinal Data Analysis):\n",
    "\n",
    "Method: Mixed-effects models or longitudinal data analysis techniques account for missing data by estimating the model parameters using all available data, assuming a specific missing data mechanism (e.g., missing at random).\n",
    "Consequences:\n",
    "Advantages: These models are powerful for repeated measures data, as they can handle missing data and provide unbiased parameter estimates when assumptions about the missing data mechanism are met.\n",
    "Disadvantages: Requires specialized software and knowledge to implement correctly. Model assumptions must be met for accurate results.\n",
    "The choice of handling missing data should depend on the nature of your data, the pattern of missingness, and the research question. Always consider the potential consequences of your chosen method and conduct sensitivity analyses to assess the impact of missing data handling on your results.\n",
    "\n",
    "It's important to note that missing data assumptions, such as MCAR, MAR, or not missing at random (NMAR), can significantly impact the validity of your analysis. Therefore, understanding the nature of missing data is critical when deciding on the appropriate handling method.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f196c2-228e-4673-8087-d02833819df3",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e7ef6-212d-4a8e-9e4b-38bb3e0fdc14",
   "metadata": {},
   "source": [
    "Post-hoc tests are used in conjunction with Analysis of Variance (ANOVA) to determine which specific groups or treatment levels are significantly different from each other when the overall ANOVA indicates that there are significant differences between groups. Post-hoc tests are necessary when you have three or more groups and want to identify pairwise differences. Some common post-hoc tests include:\n",
    "\n",
    "Tukey's Honestly Significant Difference (Tukey's HSD):\n",
    "\n",
    "When to Use: Tukey's HSD is a widely used post-hoc test and is appropriate when you have unequal group sizes and you want to control the familywise error rate (i.e., the probability of making at least one Type I error across all possible pairwise comparisons).\n",
    "Example: In a one-way ANOVA comparing the effects of three different treatments on blood pressure, Tukey's HSD can help identify which specific pairs of treatments are significantly different.\n",
    "Bonferroni Correction:\n",
    "\n",
    "When to Use: The Bonferroni correction is a conservative approach used when you have a large number of pairwise comparisons and want to control the familywise error rate. It's less powerful than Tukey's HSD but more stringent in controlling Type I errors.\n",
    "Example: In a genome-wide association study (GWAS), you're testing thousands of genetic markers for association with a trait. The Bonferroni correction can help control the overall Type I error rate in such a scenario.\n",
    "Dunnett's Test:\n",
    "\n",
    "When to Use: Dunnett's test is used when you have one control group and want to compare it to multiple treatment groups. It's designed for situations where the control group serves as the reference, and you want to identify which treatment groups differ from the control.\n",
    "Example: In a clinical trial, you have a control group receiving a placebo, and several treatment groups receiving different doses of a new drug. Dunnett's test helps determine which drug doses are significantly different from the placebo.\n",
    "Scheffé's Test:\n",
    "\n",
    "When to Use: Scheffé's test is a conservative post-hoc test suitable for unequal group sizes and complex study designs. It's used when you want to control the familywise error rate and have unequal variances between groups.\n",
    "Example: In a psychology experiment with multiple factors and interactions, Scheffé's test can be applied to determine which specific conditions lead to significant differences in performance.\n",
    "Holm-Bonferroni Method:\n",
    "\n",
    "When to Use: The Holm-Bonferroni method is a modification of the Bonferroni correction that adjusts for the multiplicity of tests. It is often used when you have a moderate number of comparisons, and it aims to balance Type I error control and statistical power.\n",
    "Example: In a study comparing the effects of different marketing strategies on sales, you have several pairwise comparisons. The Holm-Bonferroni method can be applied to control the familywise error rate while maintaining power.\n",
    "Fisher's Least Significant Difference (LSD):\n",
    "\n",
    "When to Use: Fisher's LSD is a less stringent post-hoc test that can be used when you have equal group sizes and the assumption of homogeneity of variances is met.\n",
    "Example: In an educational study comparing the performance of students in different teaching methods, Fisher's LSD can be applied to identify which teaching methods lead to significantly different outcomes.\n",
    "The choice of post-hoc test depends on the specific characteristics of your data, the assumptions being met, and your research question. It's essential to consider factors like group sizes, the number of comparisons, and the desired balance between Type I and Type II errors when selecting a post-hoc test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070789e8-f96c-4824-b3ed-a71b1a1cf5a1",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d5d27e1-f3f7-43c0-a6ca-71dc2f34669d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one-way ANOVA is statistically significant.\n",
      "F-statistic: 550.9750779440512\n",
      "p-value: 4.3819846778130536e-67\n",
      "There is a significant difference in mean weight loss between at least two diet groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for weight loss in each diet group\n",
    "diet_A = np.array([2.1, 1.9, 2.0, 1.8, 2.2, 2.1, 2.0, 1.9, 2.2, 2.1,\n",
    "                   1.8, 2.0, 2.1, 2.2, 2.0, 1.9, 1.8, 2.1, 2.0, 2.2,\n",
    "                   2.0, 1.9, 1.8, 2.1, 2.2, 2.0, 1.9, 1.8, 2.1, 2.2,\n",
    "                   2.0, 1.9, 2.2, 2.1, 2.0, 1.8, 2.2, 2.0, 1.9, 1.8,\n",
    "                   2.1, 2.0, 1.9, 2.2, 2.1, 2.0, 1.8])\n",
    "\n",
    "diet_B = np.array([1.5, 1.6, 1.7, 1.8, 1.9, 1.5, 1.7, 1.6, 1.8, 1.9,\n",
    "                   1.5, 1.6, 1.7, 1.8, 1.9, 1.5, 1.6, 1.7, 1.8, 1.9,\n",
    "                   1.5, 1.6, 1.7, 1.8, 1.9, 1.5, 1.6, 1.7, 1.8, 1.9,\n",
    "                   1.5, 1.6, 1.7, 1.8, 1.9, 1.5, 1.6, 1.7, 1.8, 1.9,\n",
    "                   1.5, 1.6, 1.7, 1.8, 1.9, 1.5, 1.6, 1.7])\n",
    "\n",
    "diet_C = np.array([2.5, 2.4, 2.7, 2.8, 2.6, 2.5, 2.7, 2.6, 2.8, 2.9,\n",
    "                   2.5, 2.4, 2.7, 2.8, 2.6, 2.5, 2.7, 2.6, 2.8, 2.9,\n",
    "                   2.5, 2.4, 2.7, 2.8, 2.6, 2.5, 2.7, 2.6, 2.8, 2.9,\n",
    "                   2.5, 2.4, 2.7, 2.8, 2.6, 2.5, 2.7, 2.6, 2.8, 2.9,\n",
    "                   2.5, 2.4, 2.7, 2.8, 2.6, 2.5, 2.7, 2.6])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The one-way ANOVA is statistically significant.\")\n",
    "    print(\"F-statistic:\", f_statistic)\n",
    "    print(\"p-value:\", p_value)\n",
    "    print(\"There is a significant difference in mean weight loss between at least two diet groups.\")\n",
    "else:\n",
    "    print(\"The one-way ANOVA is not statistically significant.\")\n",
    "    print(\"F-statistic:\", f_statistic)\n",
    "    print(\"p-value:\", p_value)\n",
    "    print(\"There is no significant difference in mean weight loss between the diet groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd70c9d-c973-44c8-a1af-6134290c4f8c",
   "metadata": {},
   "source": [
    "Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6541e0-f108-4390-bb8a-7487f3e4b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no significant main effect of software on task completion time.\n",
      "There is no significant main effect of employee experience level on task completion time.\n",
      "There is no significant interaction effect between software and employee experience level.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'Software': ['A', 'B', 'C'] * 30,\n",
    "    'Experience': ['Novice', 'Experienced'] * 45,\n",
    "    'Time': np.random.uniform(10, 30, 90)  # Replace with your actual time data\n",
    "})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ Software * Experience', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract F-statistics and p-values\n",
    "software_f = anova_table['PR(>F)']['Software']\n",
    "experience_f = anova_table['PR(>F)']['Experience']\n",
    "interaction_f = anova_table['PR(>F)']['Software:Experience']\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if software_f < alpha:\n",
    "    print(\"There is a significant main effect of software on task completion time.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of software on task completion time.\")\n",
    "\n",
    "if experience_f < alpha:\n",
    "    print(\"There is a significant main effect of employee experience level on task completion time.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of employee experience level on task completion time.\")\n",
    "\n",
    "if interaction_f < alpha:\n",
    "    print(\"There is a significant interaction effect between software and employee experience level.\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between software and employee experience level.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bb397-edb9-4314-b7b4-ff86f5ccaee5",
   "metadata": {},
   "source": [
    "Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231a657-0271-4a11-9f86-37275bbe73b5",
   "metadata": {},
   "source": [
    "To determine if there are significant differences in test scores between the control group (traditional teaching method) and the experimental group (new teaching method), you can perform a two-sample t-test in Python. If the results are significant, you can then follow up with a post-hoc test to determine which group(s) differ significantly. Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70483efb-8392-40c8-ab94-b98b6bb800b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two-sample t-test is statistically significant.\n",
      "t-statistic: -7.08158389243351\n",
      "p-value: 2.2661718617417162e-10\n",
      "There is a significant difference in test scores between the control and experimental groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for test scores in control and experimental groups\n",
    "control_group = np.array([80, 85, 78, 90, 92, 82, 75, 88, 87, 79,\n",
    "                          85, 72, 89, 94, 86, 83, 78, 91, 77, 81,\n",
    "                          84, 87, 88, 80, 91, 76, 79, 85, 93, 82,\n",
    "                          89, 90, 83, 81, 84, 86, 88, 87, 92, 82,\n",
    "                          80, 85, 78, 90, 92, 82, 75, 88, 87, 79])\n",
    "\n",
    "experimental_group = np.array([90, 92, 95, 88, 87, 93, 96, 84, 91, 97,\n",
    "                              92, 88, 95, 84, 89, 92, 94, 90, 86, 88,\n",
    "                              93, 87, 89, 92, 95, 91, 96, 93, 88, 94,\n",
    "                              91, 90, 87, 92, 94, 95, 88, 91, 86, 88,\n",
    "                              93, 94, 89, 90, 92, 86, 85, 93, 91])\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The two-sample t-test is statistically significant.\")\n",
    "    print(\"t-statistic:\", t_statistic)\n",
    "    print(\"p-value:\", p_value)\n",
    "    print(\"There is a significant difference in test scores between the control and experimental groups.\")\n",
    "else:\n",
    "    print(\"The two-sample t-test is not statistically significant.\")\n",
    "    print(\"t-statistic:\", t_statistic)\n",
    "    print(\"p-value:\", p_value)\n",
    "    print(\"There is no significant difference in test scores between the groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e109f4-c856-4a1a-8636-0f2096e78406",
   "metadata": {},
   "source": [
    "Follow Up with a Post-Hoc Test (if significant):\n",
    "If the two-sample t-test is statistically significant and you want to determine which group(s) differ significantly, you can consider post-hoc tests such as Tukey's HSD or Bonferroni correction. However, in this example, there are only two groups (control and experimental), and a post-hoc test may not be necessary unless you have more than two treatment groups to compare. If you have multiple groups to compare, you can perform the post-hoc test using appropriate Python libraries like statsmodels or scipy.stats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73271d4-c64b-4a3d-ad89-6d62d479dab9",
   "metadata": {},
   "source": [
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf8a88-023e-4d83-8c32-2f54c771a304",
   "metadata": {},
   "source": [
    "A repeated measures ANOVA is typically used when you have multiple measurements on the same subjects or items, such as repeated measurements taken on different days. In your case, you want to determine if there are significant differences in the average daily sales of three retail stores (Store A, Store B, and Store C) over 30 days.\n",
    "\n",
    "Here's how you can perform a repeated measures ANOVA in Python using the statsmodels library and, if the results are significant, follow up with a post-hoc test:\n",
    "\n",
    "Perform Repeated Measures ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69eca08-185e-4472-8344-288f70063fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data for repeated measures ANOVA\n",
    "data = pd.DataFrame({\n",
    "    'Day': list(range(1, 31)) * 3,  # 30 days, repeated for three stores\n",
    "    'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30,\n",
    "    'Sales': [100, 105, 98, 112, 115, 110, 90, 105, 103, 97,\n",
    "              105, 108, 113, 118, 100, 105, 98, 112, 115, 110,\n",
    "              90, 105, 103, 97, 105, 108, 113, 118, 100, 105]\n",
    "})\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "model = ols('Sales ~ C(Store)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA results\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9570f-01bf-4021-b82a-adbf8a4a591a",
   "metadata": {},
   "source": [
    "Interpret the Results:\n",
    "\n",
    "Check the ANOVA table for the F-statistic and p-value. If the p-value is less than your chosen significance level (alpha), you can conclude that there are significant differences in sales between the stores over the 30 days.\n",
    "Follow Up with a Post-Hoc Test (if significant):\n",
    "\n",
    "If the repeated measures ANOVA reveals significant differences, you can follow up with a post-hoc test to determine which stores differ significantly. Typically, for repeated measures data, a post-hoc test like the Bonferroni correction or Tukey's HSD can be used. Here's an example using Tukey's HSD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7bb31-55af-485a-8d00-dff794dda7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "posthoc = pairwise_tukeyhsd(data['Sales'], data['Store'], alpha=0.05)\n",
    "\n",
    "# Print the post-hoc test results\n",
    "print(posthoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde383e-77f4-41cd-8190-510b264f564d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a44d1-441c-45e5-bb92-20a22fc13a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
